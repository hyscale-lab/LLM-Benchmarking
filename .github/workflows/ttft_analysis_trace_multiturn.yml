name: TTFT Analysis Experiment

on:
  workflow_dispatch:
    inputs:
      run_trace:
        description: 'Run Trace Benchmark'
        type: boolean
        default: true
      run_multiturn:
        description: 'Run Multiturn Benchmark'
        type: boolean
        default: true

env:
  CLOUDFLARE_ACCOUNT_ID: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}
  CLOUDFLARE_AI_TOKEN: ${{ secrets.CLOUDFLARE_AI_TOKEN }}
  GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
  GROQ_API_KEY: ${{ secrets.GROQ_API_KEY }}
  HYPERBOLIC_API: ${{ secrets.HYPERBOLIC_API }}
  OPEN_AI_API: ${{ secrets.OPEN_AI_API }}
  PERPLEXITY_AI_API: ${{ secrets.PERPLEXITY_AI_API }}
  TOGETHER_AI_API: ${{ secrets.TOGETHER_AI_API }}
  ANTHROPIC_API: ${{ secrets.ANTHROPIC_API }}
  AZURE_LLAMA_70B_API: ${{ secrets.AZURE_LLAMA_70B_API }}
  AZURE_LLAMA_8B_API: ${{ secrets.AZURE_LLAMA_70B_API }}
  AWS_BEDROCK_ACCESS_KEY_ID: ${{ secrets.AWS_BEDROCK_ACCESS_KEY_ID }}
  AWS_BEDROCK_SECRET_ACCESS_KEY: ${{ secrets.AWS_BEDROCK_SECRET_ACCESS_KEY }}
  AWS_BEDROCK_REGION: ${{ secrets.AWS_BEDROCK_REGION }}
  AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
  AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
  AWS_REGION: ${{ secrets.AWS_REGION }}
  MISTRAL_LARGE_API: ${{ secrets.MISTRAL_LARGE_API }}
  AZURE_AI_ENDPOINT: ${{ secrets.AZURE_AI_ENDPOINT }}
  AZURE_AI_API_KEY: ${{ secrets.AZURE_AI_API_KEY }}
  AZURE_OPENAI_ENDPOINT: ${{ secrets.AZURE_OPENAI_ENDPOINT }}

jobs:
  trace-benchmark:
    name: Trace Benchmark
    runs-on: ubuntu-latest

    if: ${{ (github.event_name == 'workflow_dispatch' && inputs.run_trace == true) || (github.event_name == 'schedule' && github.event.schedule == '0 0 * * 2') }}
    steps:
    - name: Check out repository
      uses: actions/checkout@v3
    
    - name: Set up Python 3.12
      uses: actions/setup-python@v4
      with:
        python-version: 3.12
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Setup Trace Input
      env:
        GH_TOKEN: ${{ github.token }}
      run: |
        gh release download input-type-v2.0-ttft-analysis -p "trace_conv_sampled.json"
        echo "TRACE_DATASET_PATH=${{ github.workspace }}/trace_conv_sampled.json" >> $GITHUB_ENV

    - name: Run Trace Experiment
      run: python -u main.py -c benchmarking/experiments/compare_providers_streaming_trace.json

    - name: Upload Trace Logs
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: logs-trace
        path: |
          ${{ github.workspace }}/trace/proxy/*.log
          ${{ github.workspace }}/trace/*.result
        if-no-files-found: warn  # Prevents failure if logs weren't created yet

    - name: Upload Experiment Outputs
      if: success()
      uses: actions/upload-artifact@v4
      with:
        name: results-trace
        path: ${{ github.workspace }}/benchmark_graph

  multiturn-benchmark:
    name: Multiturn Benchmark
    runs-on: ubuntu-latest

    if: ${{ (github.event_name == 'workflow_dispatch' && inputs.run_multiturn == true) || (github.event_name == 'schedule' && github.event.schedule == '0 0 * * 3') }}
    steps:
    - name: Check out repository
      uses: actions/checkout@v3
    
    - name: Set up Python 3.12
      uses: actions/setup-python@v4
      with:
        python-version: 3.12
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Setup Multiturn Input
      env:
        GH_TOKEN: ${{ github.token }}
      run: |
        gh release download input-type-v2.0-ttft-analysis -p "multiturn_no_cache.json"
        echo "MULTITURN_DATASET_PATH=${{ github.workspace }}/multiturn_no_cache.json" >> $GITHUB_ENV

    - name: Run Multiturn Experiment
      run: python -u main.py -c benchmarking/experiments/compare_providers_streaming_multiturn.json

    - name: Upload Experiment Outputs
      if: success()
      uses: actions/upload-artifact@v4
      with:
        name: results-multiturn
        path: ${{ github.workspace }}/benchmark_graph

  download-artifacts:
    runs-on: ubuntu-latest
    needs: [trace-benchmark, multiturn-benchmark]

    # Run if at least one of the previous jobs succeed
    if: always() && (needs.trace-benchmark.result == 'success' || needs.multiturn-benchmark.result == 'success')
    steps:
      - name: Download Artifacts
        uses: actions/download-artifact@v4
        with:
          path: artifacts

      - name: List Downloaded Artifacts
        run: ls -R artifacts
