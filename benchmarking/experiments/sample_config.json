{
    "providers": ["vLLM"],  
    "models": ["common-model"],
    "num_requests": 1,
    "input_tokens": 10,
    "streaming": true,
    "max_output": 1500,
    "verbose": true
}
